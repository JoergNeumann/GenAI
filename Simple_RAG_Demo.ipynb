{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOt5Hl7v1wvLQ+u+J1ZDISy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoergNeumann/GenAI/blob/main/Simple_RAG_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Einfache RAG (Retrieval Augmented Generation) Demo"
      ],
      "metadata": {
        "id": "wJZfhXg5RXpg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Libraries importieren"
      ],
      "metadata": {
        "id": "634AwTE3RMoS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aprQBIybgqIs"
      },
      "outputs": [],
      "source": [
        "!pip install -qU \\\n",
        "    langchain==0.0.354 \\\n",
        "    openai==1.6.1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mit OpenAI via LangChain verbinden"
      ],
      "metadata": {
        "id": "92iPqT0ORrdZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwKl9hrrgqIv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "975f6d7a-3b2d-4650-deb2-356364939280"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# OpenAI API Key aus Colab Secret auslesen\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Chat erstellen\n",
        "chat = ChatOpenAI(\n",
        "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
        "    model='gpt-3.5-turbo'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Abfrage ohne RAG"
      ],
      "metadata": {
        "id": "ZrFQyw3wTsnu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xI-u_abtgqIw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f2d2513-d759-4774-cfa7-e4644f3fe24d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm sorry, but I couldn't find any specific information about \"Llama 2.\" It is possible that you may be referring to something specific or it could be a typo. If you can provide more context or clarify your question, I'll be happy to help.\n"
          ]
        }
      ],
      "source": [
        "from langchain.schema import (\n",
        "    SystemMessage,\n",
        "    HumanMessage,\n",
        "    AIMessage\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    HumanMessage(content=\"Can you tell me about the LLMChain in LangChain?\")\n",
        "]\n",
        "\n",
        "res = chat(messages)\n",
        "print(res.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kontextinformationen bereitstellen"
      ],
      "metadata": {
        "id": "1swfqPRgbHe0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "p9tkafGmgqIz"
      },
      "outputs": [],
      "source": [
        "llmchain_information = [\n",
        "    \"A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.\",\n",
        "    \"Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.\",\n",
        "    \"LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications.\"\n",
        "]\n",
        "source_knowledge = \"\\n\".join(llmchain_information)\n",
        "\n",
        "query = \"Can you tell me about the LLMChain in LangChain?\"\n",
        "\n",
        "augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
        "\n",
        "Contexts:\n",
        "{source_knowledge}\n",
        "\n",
        "Query: {query}\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Erweiterte Abfrage absetzen"
      ],
      "metadata": {
        "id": "9dI2S5jWbnQH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ZDsBYL1MgqI0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21cf518a-4d17-4466-ee88-20e810dbe11b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='The LLMChain in LangChain is the most common type of chain. It is a part of the LangChain framework, which is used for developing applications powered by language models. The LLMChain consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. It takes multiple input variables and uses the PromptTemplate to format them into a prompt. The formatted prompt is then passed to the model for processing. Finally, the output of the LLM (Language Model) is parsed into a final format using the OutputParser, if provided.')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# create a new user prompt\n",
        "prompt = HumanMessage(\n",
        "    content=augmented_prompt\n",
        ")\n",
        "# add to messages\n",
        "messages.append(prompt)\n",
        "\n",
        "# send to OpenAI\n",
        "res = chat(messages)\n",
        "res"
      ]
    }
  ]
}