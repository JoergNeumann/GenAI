{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQ3HK5qhH3AIRqsnFUKl0h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoergNeumann/GenAI/blob/main/Pinecone_RAG_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG (Retrieval Augmented Generation) mit Pinecone"
      ],
      "metadata": {
        "id": "wJZfhXg5RXpg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Libraries importieren"
      ],
      "metadata": {
        "id": "634AwTE3RMoS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtSMn6ONhfE2"
      },
      "outputs": [],
      "source": [
        "!pip install -qU \\\n",
        "    langchain_openai \\\n",
        "    langchain-community \\\n",
        "    langchain-pinecone \\\n",
        "    openai \\\n",
        "    datasets \\\n",
        "    pinecone \\\n",
        "    tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain Setup"
      ],
      "metadata": {
        "id": "92iPqT0ORrdZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwKl9hrrgqIv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# OpenAI API Key aus Colab Secret auslesen\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Chat erstellen\n",
        "chat = ChatOpenAI(\n",
        "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
        "    model='gpt-3.5-turbo'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnT_iU3sgqI0"
      },
      "source": [
        "Wir importieren Llama 2-Papers über das Dataset `\"jamescalam/llama-2-arxiv-papers\"`.\n",
        "Der Datenimport erfolgt mit Hilfe der Hugging Face Datasets Library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJOcJY3LgqI0"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\n",
        "    \"jamescalam/llama-2-arxiv-papers-chunked\",\n",
        "    split=\"train\"\n",
        ")\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset anzeigen"
      ],
      "metadata": {
        "id": "ZrFQyw3wTsnu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xI-u_abtgqIw"
      },
      "outputs": [],
      "source": [
        "dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pincone Setup. Erfordert einen [API key](https://app.pinecone.io)."
      ],
      "metadata": {
        "id": "1swfqPRgbHe0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9tkafGmgqIz"
      },
      "outputs": [],
      "source": [
        "from pinecone import Pinecone\n",
        "\n",
        "# API Key aus Colab Secret auslesen\n",
        "from google.colab import userdata\n",
        "api_key = userdata.get('PINECONE_API_KEY')\n",
        "\n",
        "# configure client\n",
        "pc = Pinecone(api_key=api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Index erstellen und Cloud Provider / Region wählen in der gespeichert werden soll."
      ],
      "metadata": {
        "id": "9dI2S5jWbnQH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uThclKlKgqI5"
      },
      "outputs": [],
      "source": [
        "from pinecone import ServerlessSpec\n",
        "\n",
        "spec = ServerlessSpec(\n",
        "    cloud=\"aws\", region=\"us-west-2\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6a-lOh7gqI5"
      },
      "source": [
        "Index initialisieren. Da wir OpenAI's `text-embedding-ada-002`-Model verwenden, stellen wir die `dimension` auf `1536`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdqZ9OPlgqI6"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "index_name = 'llama-2-rag'\n",
        "existing_indexes = [\n",
        "    index_info[\"name\"] for index_info in pc.list_indexes()\n",
        "]\n",
        "\n",
        "# check if index already exists (it shouldn't if this is first time)\n",
        "if index_name not in existing_indexes:\n",
        "    # if does not exist, create index\n",
        "    pc.create_index(\n",
        "        index_name,\n",
        "        dimension=1536,  # dimensionality of ada 002\n",
        "        metric='dotproduct',\n",
        "        spec=spec\n",
        "    )\n",
        "    # wait for index to be initialized\n",
        "    while not pc.describe_index(index_name).status['ready']:\n",
        "        time.sleep(1)\n",
        "\n",
        "# connect to index\n",
        "index = pc.Index(index_name)\n",
        "time.sleep(1)\n",
        "# view index stats\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vector Embeddings mit Hilfe von `text-embedding-ada-002` erzeugen"
      ],
      "metadata": {
        "id": "lnv8lizzl0TY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8epqnCR3gqI6"
      },
      "outputs": [],
      "source": [
        "from langchain_community.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "embed_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
        "\n",
        "texts = [\n",
        "    'this is the first chunk of text',\n",
        "    'then another second chunk of text is here'\n",
        "]\n",
        "\n",
        "res = embed_model.embed_documents(texts)\n",
        "len(res), len(res[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wir bekommen für die 2 Chunks 2 x 1536-dimensionale embeddings.\n",
        "Nun können wir die Texte indexieren, Embeddings erzeugen und diese speichern."
      ],
      "metadata": {
        "id": "RuvNoDojots4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYERROX6gqI7"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm  # for progress bar\n",
        "\n",
        "data = dataset.to_pandas()  # this makes it easier to iterate over the dataset\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "for i in tqdm(range(0, len(data), batch_size)):\n",
        "    i_end = min(len(data), i+batch_size)\n",
        "    # get batch of data\n",
        "    batch = data.iloc[i:i_end]\n",
        "    # generate unique ids for each chunk\n",
        "    ids = [f\"{x['doi']}-{x['chunk-id']}\" for i, x in batch.iterrows()]\n",
        "    # get text to embed\n",
        "    texts = [x['chunk'] for _, x in batch.iterrows()]\n",
        "    # embed text\n",
        "    embeds = embed_model.embed_documents(texts)\n",
        "    # get metadata to store in Pinecone\n",
        "    metadata = [\n",
        "        {'text': x['chunk'],\n",
        "         'source': x['source'],\n",
        "         'title': x['title']} for i, x in batch.iterrows()\n",
        "    ]\n",
        "    # add to Pinecone\n",
        "    index.upsert(vectors=zip(ids, embeds, metadata))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Erzeugten Index untersuchen"
      ],
      "metadata": {
        "id": "e2qW4eLhvALG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJicTpX5gqI7"
      },
      "outputs": [],
      "source": [
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain VectorStore erzeugen"
      ],
      "metadata": {
        "id": "Mgw8vWBov-4f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aq_rDKXygqI8"
      },
      "outputs": [],
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "index = pc.Index(index_name)\n",
        "vectorstore = PineconeVectorStore(index=index, embedding=OpenAIEmbeddings())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Index nach Frage durchsuchen"
      ],
      "metadata": {
        "id": "Z4YP3b6pwZhg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RA2LAedvgqI8"
      },
      "outputs": [],
      "source": [
        "query = \"What is so special about Llama 2?\"\n",
        "\n",
        "vectorstore.similarity_search(query, k=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Auf dem VectorStore wird nun die Suche ausgeführt und aus dem Ergebnis ein Agumented Prompt erstellt."
      ],
      "metadata": {
        "id": "1yCrkO0VwxIN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kg6UHxwgqI8"
      },
      "outputs": [],
      "source": [
        "def augment_prompt(query: str):\n",
        "    # get top 3 results from knowledge base\n",
        "    results = vectorstore.similarity_search(query, k=3)\n",
        "    # get the text from the results\n",
        "    source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
        "    # feed into an augmented prompt\n",
        "    augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
        "\n",
        "    Contexts:\n",
        "    {source_knowledge}\n",
        "\n",
        "    Query: {query}\"\"\"\n",
        "    return augmented_prompt\n",
        "\n",
        "print(augment_prompt(query))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Abfrage absetzen"
      ],
      "metadata": {
        "id": "Rde8mrtsyDzy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_rroaUSgqI-"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import (\n",
        "    HumanMessage\n",
        ")\n",
        "\n",
        "# create a new user prompt\n",
        "messages = [\n",
        "    HumanMessage(content = augment_prompt(query) )\n",
        "]\n",
        "\n",
        "res = chat(messages)\n",
        "\n",
        "print(res.content)"
      ]
    }
  ]
}