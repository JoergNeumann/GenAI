{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNv2kqRWdSL0sgUfzyybrw5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoergNeumann/GenAI/blob/main/RAG%20Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RAG Demo\n",
        "Demonstriert die Implementierung des RAG-Pattern mit der OpenAI API.\n",
        "\n",
        "##Setup"
      ],
      "metadata": {
        "id": "V15ZG_CLJdld"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bt7NVODrJYh1"
      },
      "outputs": [],
      "source": [
        "!pip install openai numpy scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## API-Zugriff konfigurieren"
      ],
      "metadata": {
        "id": "h--kGEU6JrgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "import time\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# OpenAI API Key aus Colab Secret auslesen und OpenAI Client erstellen\n",
        "from google.colab import userdata\n",
        "api_key = userdata.get('OPENAI_API_KEY')\n",
        "client = openai.OpenAI(api_key=api_key)"
      ],
      "metadata": {
        "id": "OUJ-4tCGKBse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Beispieltexte erstellen"
      ],
      "metadata": {
        "id": "QJTuEpHMKKk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "knowledge_base = [\n",
        "    \"Python ist eine interpretierte, hochgradig abstrahierte Programmiersprache.\",\n",
        "    \"Die OpenAI API ermöglicht den Zugriff auf fortschrittliche KI-Modelle.\",\n",
        "    \"Machine Learning ist ein Teilgebiet der künstlichen Intelligenz.\",\n",
        "    # Weitere Texte hinzufügen ...\n",
        "]"
      ],
      "metadata": {
        "id": "Q4XU8nScKSSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embeddings erzeugen"
      ],
      "metadata": {
        "id": "i_JR-ZTFKZzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(text):\n",
        "    response = client.embeddings.create(\n",
        "        input=text,\n",
        "        model=\"text-embedding-ada-002\"\n",
        "    )\n",
        "    print(text)\n",
        "    print(response.data[0].embedding)\n",
        "    return response.data[0].embedding\n",
        "\n",
        "knowledge_embeddings = [get_embedding(doc) for doc in knowledge_base]\n"
      ],
      "metadata": {
        "id": "CAKr-IVUKbv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Benutzerabfrage erstellen / passende Texte finden"
      ],
      "metadata": {
        "id": "ZF7ETh2oKkdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_most_relevant_document(user_query):\n",
        "    query_embedding = get_embedding(user_query)\n",
        "    similarities = cosine_similarity(\n",
        "        [query_embedding],\n",
        "        knowledge_embeddings\n",
        "    )\n",
        "    most_relevant_index = np.argmax(similarities)\n",
        "    return knowledge_base[most_relevant_index]\n",
        "\n",
        "user_query = \"Was ist maschinelles Lernen?\"\n",
        "relevant_document = find_most_relevant_document(user_query)\n",
        "print(f\"Relevantestes Dokument: {relevant_document}\")"
      ],
      "metadata": {
        "id": "Yh3TTY8sKuGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Antwort generieren"
      ],
      "metadata": {
        "id": "SoWT6ub9K-5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(user_query, context):\n",
        "    prompt = (\n",
        "        f\"Beantworte die Frage basierend auf dem unten stehenden Kontext. \\\n",
        "        Wenn die Frage nicht mit den bereitgestellten Informationen beantwortet werden kann, antworte mit 'Ich weiß es nicht'.\\n\\n\"\n",
        "        f\"Kontext: {context}\\n\\n\"\n",
        "        f\"Frage: {user_query}\"\n",
        "    )\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"Du bist ein hilfreicher Assistent.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        max_tokens=150,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "response = generate_response(user_query, relevant_document)\n",
        "print(f\"Antwort: {response}\")"
      ],
      "metadata": {
        "id": "UuvI854yLA1S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}